{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
    "See follow links for more information\n",
    "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "df = pd.get_dummies(df, prefix=['thal'])\n",
    "feature_names = df.drop(['target'], axis=1).columns\n",
    "\n",
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "\n",
    "x_train = train_df.drop(['target'], axis=1).values\n",
    "y_train = train_df['target'].values\n",
    "\n",
    "x_test = test_df.drop(['target'], axis=1).values\n",
    "y_test = test_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>target</th>\n",
       "      <th>thal_1</th>\n",
       "      <th>thal_2</th>\n",
       "      <th>thal_fixed</th>\n",
       "      <th>thal_normal</th>\n",
       "      <th>thal_reversible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "136   54    1   2       192   283    0        2      195      0      0.0   \n",
       "232   58    0   4       170   225    1        2      146      1      2.8   \n",
       "233   56    1   2       130   221    0        2      163      0      0.0   \n",
       "184   46    1   4       120   249    0        2      144      0      0.8   \n",
       "84    55    0   2       135   250    0        2      161      0      1.4   \n",
       "\n",
       "     slope  ca  target  thal_1  thal_2  thal_fixed  thal_normal  \\\n",
       "136      1   1       0       0       0           0            0   \n",
       "232      2   2       1       0       0           1            0   \n",
       "233      1   0       0       0       0           0            0   \n",
       "184      1   0       0       0       0           0            0   \n",
       "84       2   0       0       0       0           0            1   \n",
       "\n",
       "     thal_reversible  \n",
       "136                1  \n",
       "232                0  \n",
       "233                1  \n",
       "184                1  \n",
       "84                 0  "
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Gini impurity (local entropy) of a label sequence\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1 - sum([(i / N) ** 2 for i in hist])\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Entropy of a label sequence\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / np.sum(hist)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier=True,\n",
    "        max_depth=None,\n",
    "        max_features=None,\n",
    "        criterion=\"entropy\",\n",
    "        seed=None,\n",
    "    ):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "        self.feature_values = []\n",
    "        if not classifier and criterion in [\"gini\", \"entropy\"]:\n",
    "            raise ValueError(\n",
    "                \"{} is a valid criterion only when classifier = True.\".format(criterion)\n",
    "            )\n",
    "        if classifier and criterion == \"mse\":\n",
    "            raise ValueError(\"`mse` is a valid criterion only when classifier = False.\")\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None\n",
    "        self.max_features = X.shape[1] if not self.max_features else min(self.max_features, X.shape[1])\n",
    "        self.root = self._grow(X, Y)\n",
    "\n",
    "    def predict(self, X):   \n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def predict_class_probs(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to return the class probabilities for the\n",
    "        examples in `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N, n_classes)`\n",
    "            The class probabilities predicted for each example in `X`.\n",
    "        \"\"\"\n",
    "        assert self.classifier, \"`predict_class_probs` undefined for classifier = False\"\n",
    "        return np.array([self._traverse(x, self.root, prob=True) for x in X])\n",
    "\n",
    "    def _grow(self, X, Y, cur_depth=0):\n",
    "        # if all labels are the same, return a leaf\n",
    "        if len(set(Y)) == 1:\n",
    "            if self.classifier:\n",
    "                prob = np.zeros(self.n_classes)\n",
    "                prob[Y[0]] = 1.0\n",
    "            return Leaf(prob) if self.classifier else Leaf(Y[0])\n",
    "\n",
    "        # if we have reached max_depth, return a leaf\n",
    "        if cur_depth >= self.max_depth:\n",
    "            v = np.mean(Y, axis=0)\n",
    "            if self.classifier:\n",
    "                v = np.bincount(Y, minlength=self.n_classes) / len(Y)\n",
    "            return Leaf(v)\n",
    "\n",
    "        cur_depth += 1\n",
    "        self.depth = max(self.depth, cur_depth)\n",
    "\n",
    "        N, M = X.shape\n",
    "        feat_idxs = np.random.choice(M, self.max_features, replace=False)\n",
    "\n",
    "        # greedily select the best split according to `criterion`\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        self.feature_values.append(feat)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left = self._grow(X[l, :], Y[l], cur_depth)\n",
    "        right = self._grow(X[r, :], Y[r], cur_depth)\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        \"\"\"\n",
    "        Find the optimal split rule (feature index and split threshold) for the\n",
    "        data according to `self.criterion`.\n",
    "        \"\"\"\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "        for i in feat_idxs:\n",
    "            vals = X[:, i]    \n",
    "            levels = np.unique(vals)\n",
    "            thresholds = (levels[:-1] + levels[1:]) / 2 if len(levels) > 1 else levels\n",
    "            gains = np.array([self._impurity_gain(Y, t, vals) for t in thresholds])\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _impurity_gain(self, Y, split_thresh, feat_values):\n",
    "        \"\"\"\n",
    "        Compute the impurity gain associated with a given split.\n",
    "        IG(split) = loss(parent) - weighted_avg[loss(left_child), loss(right_child)]\n",
    "        \"\"\"\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            loss = gini\n",
    "        parent_loss = loss(Y)\n",
    "\n",
    "        # generate split\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(Y)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        e_l, e_r = loss(Y[left]), loss(Y[right])\n",
    "        child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # impurity gain is difference in loss before vs. after split\n",
    "        ig = parent_loss - child_loss\n",
    "        return ig\n",
    "\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        if isinstance(node, Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value if prob else node.value.argmax()\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        \"\"\"\n",
    "        `value` is an array of class probabilities if classifier is True, else\n",
    "        the mean of the region\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "def bootstrap_sample(X, Y):\n",
    "    N, M = X.shape\n",
    "    idxs = np.random.choice(N, N, replace=True)\n",
    "    return X[idxs], Y[idxs]\n",
    "\n",
    "\n",
    "def accuracy(y_gt, y_pred):\n",
    "    return np.sum(y_gt == y_pred) / y_gt.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree\n",
      "criterion = gini\n",
      "max depth = 3\n",
      "acc: 0.78\n"
     ]
    }
   ],
   "source": [
    "tree3 = DecisionTree(max_depth=3, criterion='gini')\n",
    "tree3.fit(x_train, y_train)\n",
    "inference = tree3.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"Decision tree\")\n",
    "print(f'criterion = {tree3.criterion}')\n",
    "print(f'max depth = {tree3.max_depth}')\n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree\n",
      "criterion = gini\n",
      "max depth = 10\n",
      "acc: 0.72\n"
     ]
    }
   ],
   "source": [
    "tree10 = DecisionTree(max_depth=10, criterion='gini')\n",
    "tree10.fit(x_train, y_train)\n",
    "inference = tree10.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"Decision tree\")\n",
    "print(f'criterion = {tree10.criterion}')\n",
    "print(f'max depth = {tree10.max_depth}')\n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree\n",
      "criterion = entropy\n",
      "max depth = 3\n",
      "acc: 0.76\n"
     ]
    }
   ],
   "source": [
    "tree3 = DecisionTree(max_depth=3, criterion='entropy')\n",
    "tree3.fit(x_train, y_train)\n",
    "inference = tree3.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"Decision tree\")\n",
    "print(f'criterion = {tree3.criterion}')\n",
    "print(f'max depth = {tree3.max_depth}')\n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree\n",
      "criterion = entropy\n",
      "max depth = 10\n",
      "acc: 0.78\n"
     ]
    }
   ],
   "source": [
    "tree10 = DecisionTree(max_depth=10, criterion='entropy')\n",
    "tree10.fit(x_train, y_train)\n",
    "inference = tree10.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"Decision tree\")\n",
    "print(f'criterion = {tree10.criterion}')\n",
    "print(f'max depth = {tree10.max_depth}') \n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splitting_data = tree10.feature_values\n",
    "f_index, f_counts = np.unique(splitting_data, return_counts=True)\n",
    "feature_importance = np.zeros(x_train.shape[1])\n",
    "for i, j in enumerate(f_counts):\n",
    "    feature_importance[f_index[i]] = j\n",
    "x_pos = [i for i, _ in enumerate(feature_importance)]\n",
    "\n",
    "plt.title('Feature importance')\n",
    "plt.barh(x_pos, feature_importance)\n",
    "plt.ylabel('feature names')\n",
    "plt.xlabel('counts')\n",
    "plt.xticks(np.arange(max(f_counts)+1))\n",
    "plt.yticks(x_pos, feature_names)\n",
    "plt.gca().grid(axis='x', which='major')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.clfs = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        self.clfs = []\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_estimators):\n",
    "            clf = DecisionTree()\n",
    "            clf.fit(X, y)\n",
    "            min_error = float(\"inf\")\n",
    "            # greedy search to find best threshold and feature\n",
    "            for feature_i in range(n_features):\n",
    "                X_column = X[:, feature_i]\n",
    "                thresholds = np.unique(X_column)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    # predict with polarity 1\n",
    "                    p = 1\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[X_column < threshold] = -1\n",
    "\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    misclassified = w[y != predictions]\n",
    "                    error = sum(misclassified)\n",
    "\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # store the best configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_idx = feature_i\n",
    "                        min_error = error\n",
    "\n",
    "            # calculate alpha\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # calculate predictions and update weights\n",
    "            predictions = clf.predict(X)\n",
    "\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 10\n",
      "acc: 0.74\n"
     ]
    }
   ],
   "source": [
    "clf = Adaboost(n_estimators=10)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f'n_estimators = {clf.n_estimators}') \n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100\n",
      "acc: 0.74\n"
     ]
    }
   ],
   "source": [
    "clf = Adaboost(n_estimators=100)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f'n_estimators = {clf.n_estimators}')\n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(\n",
    "        self, n_estimators, max_depth, max_features, classifier=True, criterion=\"entropy\", boostrap=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        An ensemble (forest) of decision trees where each split is calculated\n",
    "        using a random subset of the features in the input.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int\n",
    "            The number of individual decision trees to use within the ensemble.\n",
    "        max_depth: int or None\n",
    "            The depth at which to stop growing each decision tree. If None,\n",
    "            grow each tree until the leaf nodes are pure.\n",
    "        max_features : int\n",
    "            The number of features to sample on each split.\n",
    "        classifier : bool\n",
    "            Whether `Y` contains class labels or real-valued targets. Default\n",
    "            is True.\n",
    "        criterion : {'entropy', 'gini', 'mse'}\n",
    "            The error criterion to use when calculating splits for each weak\n",
    "            learner. When ``classifier = False``, valid entries are {'mse'}.\n",
    "            When ``classifier = True``, valid entries are {'entropy', 'gini'}.\n",
    "            Default is 'entropy'.\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.boostrap = boostrap\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Create `n_estimators`-worth of bootstrapped samples from the training data\n",
    "        and use each to fit a separate decision tree.\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            if self.boostrap:\n",
    "                X_samp, Y_samp = bootstrap_sample(X, Y)\n",
    "            else:\n",
    "                X_samp, Y_samp = X, Y\n",
    "            tree = DecisionTree(\n",
    "                max_features=self.max_features,\n",
    "                max_depth=self.max_depth,\n",
    "                criterion=self.criterion,\n",
    "                classifier=self.classifier,\n",
    "            )\n",
    "            tree.fit(X_samp, Y_samp)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target value for each entry in `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            Model predictions for each entry in `X`.\n",
    "        \"\"\"\n",
    "        tree_preds = np.array([[t._traverse(x, t.root) for x in X] for t in self.trees])\n",
    "        print(\"Random forest\")\n",
    "        print(f'n estimators = {self.n_estimators}')\n",
    "        print(f'max features = {self.max_features}')\n",
    "        print(f'boostrap     = {self.boostrap}')\n",
    "        print(f'criterion    = {self.criterion}')\n",
    "        print(f'max depth    = {self.max_depth}')\n",
    "        return self._vote(tree_preds)\n",
    "\n",
    "    def _vote(self, predictions):\n",
    "        \"\"\"\n",
    "        Return the aggregated prediction across all trees in the RF for each problem.\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictions : :py:class:`ndarray <numpy.ndarray>` of shape `(n_estimators, N)`\n",
    "            The array of predictions from each decision tree in the RF for each\n",
    "            of the `N` problems in `X`.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            If classifier is True, the class label predicted by the majority of\n",
    "            the decision trees for each problem in `X`. If classifier is False,\n",
    "            the average prediction across decision trees on each problem.\n",
    "        \"\"\"\n",
    "        if self.classifier:\n",
    "            out = [np.bincount(x).argmax() for x in predictions.T]\n",
    "        else:\n",
    "            out = [np.mean(x) for x in predictions.T]\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "n estimators = 10\n",
      "max features = 5\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc 0.75\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(\n",
    "                n_estimators=10,\n",
    "                max_depth=None,\n",
    "                max_features=int(np.ceil(np.sqrt(x_train.shape[1]))),\n",
    "                criterion='gini',\n",
    "                boostrap=True\n",
    "            )\n",
    "rf.fit(x_train, y_train)\n",
    "inference = rf.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"acc\", acc)\n",
    "print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "n estimators = 100\n",
      "max features = 5\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc 0.79\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(\n",
    "               n_estimators=100,\n",
    "               max_depth=None,\n",
    "               max_features=int(np.ceil(np.sqrt(x_train.shape[1]))),\n",
    "               criterion='gini',\n",
    "               boostrap=True\n",
    "           )\n",
    "rf.fit(x_train, y_train)\n",
    "inference = rf.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"acc\", acc)\n",
    "print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "n estimators = 10\n",
      "max features = 5\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc 0.79\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(\n",
    "                n_estimators=10,\n",
    "                max_depth=None,\n",
    "                max_features=int(np.ceil(np.sqrt(x_train.shape[1]))),\n",
    "                criterion='gini',\n",
    "                boostrap=True\n",
    "            )\n",
    "rf.fit(x_train, y_train)\n",
    "inference = rf.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"acc\", acc)\n",
    "print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "n estimators = 10\n",
      "max features = 17\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc 0.74\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(\n",
    "                n_estimators=10,\n",
    "                max_depth=None,\n",
    "                max_features=x_train.shape[1],\n",
    "                criterion='gini',\n",
    "                boostrap=True\n",
    "            )\n",
    "rf.fit(x_train, y_train)\n",
    "inference = rf.predict(x_test)\n",
    "acc = accuracy(y_test, inference)\n",
    "print(\"acc\", acc)\n",
    "print(\"======================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.\n",
    "Try you best to get highest test accuracy score by \n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "n estimators = 10\n",
      "max features = 17\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "Test-set accuarcy score:  0.8\n",
      "Test-set accuarcy score:  0.85\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(\n",
    "                n_estimators=10,\n",
    "                max_depth=None,\n",
    "                max_features=x_train.shape[1],\n",
    "                criterion='gini',\n",
    "                boostrap=True\n",
    "            )\n",
    "rf.fit(x_train, y_train)\n",
    "inference = rf.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, inference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result:\n",
    "## Decision Tree\n",
    "* Decision trees are easy to understand and interpret, can be visually analyzed, and rules can be easily extracted.\n",
    "* Can handle both nominal and numerical data.\n",
    "* It's better suited to handle samples with missing attributes.\n",
    "* Decision Tree is ability to handle irrelevant features.\n",
    "* Relatively fast running speed when testing datasets.\n",
    "* It can produce feasible and effective results for large data sources in a relatively short period of time.\n",
    "\n",
    "## Adaboost\n",
    "* Adaboost is good use of weak classifiers for cascading.\n",
    "* The possibility of using different classification algorithms as weak classifiers.\n",
    "* The high accuracy of AdaBoost.\n",
    "* The weights of each classifier fully considered by AdaBoost compared to the bagging algorithm and Random Forest algorithm.\n",
    "\n",
    "## Random forest\n",
    "* The underlying algorithm of the random forest is based on the CART algorithm, so it can handle both categorical and continuous data.\n",
    "* For most of the data, the random forest algorithm has a high accuracy of the proposed results.\n",
    "* It accepts high-dimensional feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
